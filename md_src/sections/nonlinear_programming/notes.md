## Notes and further reading

This section of the notes borrowed very heavily from @classText chapter 13, as well as sections of appendices 2 and 3. Writing the calculus review gave me good reason to glance again at the excellent @spivakCalculus, which is the first math book I remember ever reading for fun (really, it's beautifully written).

Of course, nonlinear programming is a broad subject, so any unit of the length given here will be missing some interesting content. I would have liked to talk a bit more about gradient descent methods, potentially tying it more explicitly to the training of neural networks and introducing stochastic gradient descent. But there was no way we'd have time for that. With even more time, I would have liked to talk more about [quasi-Newton](https://en.wikipedia.org/wiki/Quasi-Newton_method) and [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) methods.

The K-State IMSE department offers [IMSE 982](https://catalog.k-state.edu/content.php?catoid=58&navoid=11444), a full semester course on nonlinear programming theory and algorithms.
